{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":["x1RoAP4Uqdcq","Up93FARFKDl-","ZuDic3c7TE-4","5wIQIlTyQZM7"],"gpuType":"T4","authorship_tag":"ABX9TyPZ3lMguUYutLK36aNDsrvY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-ihpCHAyHyp5"},"outputs":[],"source":["!gdown 1qsjT1vfjKohCjTkU03CevOKraHJ3hlGc"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler\n","from sklearn.model_selection import train_test_split,GridSearchCV\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, roc_curve, auc, make_scorer\n","\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"NtAICMgrISAO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('/content/2023_senior_data_scientist_challenge.csv')"],"metadata":{"id":"lrXNEQxXIazq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Basic Preprocessing**"],"metadata":{"id":"x1RoAP4Uqdcq"}},{"cell_type":"code","source":["# need to fix wrong data types and fill some values for now.\n","df.info()"],"metadata":{"id":"6Td2CFyAuAG6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# datatime conversion\n","df['REGISTRATION_AT'] = pd.to_datetime(df.REGISTRATION_AT).dt.to_period('D').dt.to_timestamp()\n","df['LAST_VISIT_AT'] = pd.to_datetime(df.LAST_VISIT_AT).dt.to_period('D').dt.to_timestamp()\n","df['LAST_POST_AT'] = pd.to_datetime(df.LAST_POST_AT).dt.to_period('D').dt.to_timestamp()\n","df['LAST_LIKE_RECEIVED_AT'] = pd.to_datetime(df.LAST_LIKE_RECEIVED_AT).dt.to_period('D').dt.to_timestamp()\n","df['LAST_COMMENT_RECEIVED_AT'] = pd.to_datetime(df.LAST_COMMENT_RECEIVED_AT).dt.to_period('D').dt.to_timestamp()\n","df['LAST_LIKE_GIVEN_AT'] = pd.to_datetime(df.LAST_LIKE_GIVEN_AT).dt.to_period('D').dt.to_timestamp()\n","df['LAST_COMMENT_WRITTEN_AT'] = pd.to_datetime(df.LAST_COMMENT_WRITTEN_AT).dt.to_period('D').dt.to_timestamp()"],"metadata":{"id":"AOuCV1PercmV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# filled dates with earliest date we had for the user\n","df.LAST_LIKE_RECEIVED_AT.fillna(df['REGISTRATION_AT'],inplace=True)\n","df.LAST_COMMENT_RECEIVED_AT.fillna(df['REGISTRATION_AT'],inplace=True)\n","df.LAST_LIKE_GIVEN_AT.fillna(df['REGISTRATION_AT'],inplace=True)\n","df.LAST_COMMENT_WRITTEN_AT.fillna(df['REGISTRATION_AT'],inplace=True)\n","\n","# if no value then 0\n","df.TOTAL_LIKES_GIVEN.fillna(0,inplace=True)\n","df.TOTAL_COMMENTS_WRITTEN.fillna(0,inplace=True)"],"metadata":{"id":"QsCfy-xrujkc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **EDA**"],"metadata":{"id":"Up93FARFKDl-"}},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"T9y5X1mfJ1mw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"vVKHEOI7h25s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# mostly min dates are 2022-03-01\n","df.describe(datetime_is_numeric=True, include = 'all')"],"metadata":{"id":"cm568CYrg6si"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# perfectly balanced classes\n","df['CHURNED'].value_counts(), df['CHURNED'].astype(int).hist(bins=3,figsize=(5,5))"],"metadata":{"id":"pCe2Jow9jwiP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df2 = df.copy()\n","le = LabelEncoder()\n","for col in df2.select_dtypes(include=['object', 'bool', 'datetime64[ns, UTC]']).columns:\n","    df2[col] = le.fit_transform(df2[col]).astype('float64')\n","plt.figure(figsize=(6, 6))  # Adjust size here\n","sns.heatmap(df2.corr(method='spearman',numeric_only=True), annot=True, fmt='.2f')\n","plt.show()"],"metadata":{"id":"0vL9B0yVAJa1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Feature Engineering**"],"metadata":{"id":"ZuDic3c7TE-4"}},{"cell_type":"markdown","source":["**Feature Creation**"],"metadata":{"id":"hKSOEbIMCH3I"}},{"cell_type":"code","source":["# get current date\n","max_dataset_date = df.LAST_VISIT_AT.max()\n","max_dataset_date"],"metadata":{"id":"fk_b9tMNR60K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. Adding new time-based features\n","df['DAYS_SINCE_LAST_COMMENT_WRITTEN'] = (max_dataset_date - df['LAST_COMMENT_WRITTEN_AT']).dt.days\n","df['DAYS_SINCE_LAST_POST'] = (max_dataset_date - df['LAST_POST_AT']).dt.days\n","df['DAYS_SINCE_LAST_VISIT'] = (max_dataset_date - df['LAST_VISIT_AT']).dt.days\n","df['DAYS_SINCE_LAST_LIKE_GIVEN'] = (max_dataset_date - df['LAST_LIKE_GIVEN_AT']).dt.days\n","df['DAYS_SINCE_LAST_LIKE_RECEIVED'] = (max_dataset_date - df['LAST_LIKE_RECEIVED_AT']).dt.days\n","df['DAYS_SINCE_LAST_COMMENT_RECEIVED'] = (max_dataset_date - df['LAST_COMMENT_RECEIVED_AT']).dt.days\n","\n","# 2. Days Since Registration (Customer's lifetime)\n","df['DAYS_SINCE_REGISTRATION'] = (max_dataset_date - df['REGISTRATION_AT']).dt.days\n","\n","# Days passed since last activity (Recency)\n","df['DAYS_SINCE_LAST_ACTIVITY'] = df[['LAST_VISIT_AT', 'LAST_POST_AT',\n","                                                'LAST_LIKE_GIVEN_AT', 'LAST_COMMENT_WRITTEN_AT']].max(axis=1)\n","df['DAYS_SINCE_LAST_ACTIVITY'] = (max_dataset_date - df['DAYS_SINCE_LAST_ACTIVITY']).dt.days\n","\n","# 3. Average Visit Frequency (Frequency)\n","# Total days since registration divided by total visit count (avoid division by zero)\n","df['AVERAGE_VISIT_FREQUENCY'] = df['DAYS_SINCE_REGISTRATION'] / df['TOTAL_VISIT_COUNT'].replace(0, 1)\n","\n","# 4. Post to Visit Ratio\n","df['POST_TO_VISIT_RATIO'] = df['TOTAL_POST_COUNT'] / df['TOTAL_VISIT_COUNT'].replace(0, 1)\n","\n","# 5. Like to Comment Ratio (Given)\n","df['LIKE_TO_COMMENT_RATIO_GIVEN'] = df['TOTAL_LIKES_GIVEN'] / df['TOTAL_COMMENTS_WRITTEN'].replace(0, 1)\n","\n","# 6. Like to Comment Ratio (Received)\n","df['LIKE_TO_COMMENT_RATIO_RECEIVED'] = df['TOTAL_LIKES_RECEIVED'] / df['TOTAL_COMMENTS_RECEIVED'].replace(0, 1)\n","\n","# 7. Engagement Score\n","# Creating a simple sum of normalized key engagement metrics\n","engagement_metrics = ['TOTAL_VISIT_COUNT', 'TOTAL_POST_COUNT', 'TOTAL_LIKES_GIVEN', 'TOTAL_COMMENTS_WRITTEN']\n","df['ENGAGEMENT_SCORE'] = df[engagement_metrics].apply(lambda x: x / x.max(), axis=1).sum(axis=1)"],"metadata":{"id":"xwANbfMOSGGB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","#####################################\n","\n","---\n","\n"],"metadata":{"id":"V1vzqr69pdr6"}},{"cell_type":"markdown","source":["**Label Creation**: As, we wanna find out users that arent already churned but on risk of churning. We have created a criteria for these. Which says:\n","1. Users that are showing a decreased activity on the app.\n","2. Users that were last active 20 - 62  days (They are 33% to churn already)"],"metadata":{"id":"4MIjFupkgy7r"}},{"cell_type":"code","source":["'''\n","As, we already know users with DAYS_SINCE_LAST_VISIT > 62 have already been churned, so this doesnt need model's training to detect.\n","We are only concerned about 2 things,\n","1. Users who a not been churned.\n","2. User at the risk of churning.\n","\n","So, we will first remove the churned user's set from the data directly.\n","'''\n","# removing the churned users from the data\n","df = df[~df.CHURNED]\n","\n","# setting value for users which are at risk of churning to True\n","df.loc[(~df.CHURNED) &\n","       ((df.DAYS_SINCE_LAST_VISIT > 30) |  # half way to churn\n","        (df['ENGAGEMENT_SCORE'] < 1.2)),\n","       'CHURNED'] = True\n","      #  ((df.AVERAGE_VISIT_FREQUENCY < df.DAYS_SINCE_LAST_VISIT))),  # activity decreasing on app"],"metadata":{"id":"7Gj6ArAOgxwa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.CHURNED.value_counts()"],"metadata":{"id":"2z_hh0rGh1Z_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","To balance out the classes a little bit,\n","Keeping a random sample of 12000 from the not-churn class.\n","This would be enough to represent user segement.\n","'''\n","to_remove = df[df.CHURNED]\n","to_keep = to_remove.sample(n=7000)\n","df = df.drop(to_remove.index).append(to_keep)"],"metadata":{"id":"hnZE5JSRzbes"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","#####################################\n","\n","---\n","\n"],"metadata":{"id":"HChVK_sypZZq"}},{"cell_type":"markdown","source":["Further EDA using aggregated features."],"metadata":{"id":"8EtPA0BdNhfg"}},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"aIz2TOqtkqUZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df2 = df.copy()\n","le = LabelEncoder()\n","for col in df2.select_dtypes(include=['object', 'bool']).columns:\n","    df2[col] = le.fit_transform(df2[col]).astype('float64')\n","plt.figure(figsize=(15, 15))  # Adjust size here\n","sns.heatmap(df2.corr(method='spearman',numeric_only=True), annot=True, fmt='.2f')\n","plt.show()"],"metadata":{"id":"QE5OynLZkrWo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Removing some features based on\n","1. No contribution to label.\n","2. Collinearity.\n","2. Aggregated to new features."],"metadata":{"id":"ljkXkYWHeVuY"}},{"cell_type":"code","source":["df = df.drop(columns=['USER_ID','REGISTRATION_AT','LAST_VISIT_AT','LAST_POST_AT','LAST_LIKE_GIVEN_AT','LAST_LIKE_RECEIVED_AT',\n","                      'LAST_COMMENT_RECEIVED_AT','LAST_COMMENT_WRITTEN_AT','AVERAGE_VISIT_FREQUENCY','DAYS_SINCE_LAST_ACTIVITY'], errors='ignore')"],"metadata":{"id":"KeCR9lkwdvf8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's see how does the data distribution looks now (focusing on outliers)."],"metadata":{"id":"6N6T7lg2NQVz"}},{"cell_type":"code","source":["df.describe()"],"metadata":{"id":"RTQreHnOiBGL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting box plots for each column to visually identify outliers\n","df2 = df.copy()\n","\n","df2.drop('CHURNED',axis = 1,inplace=True)\n","plt.figure(figsize=(20, 15))\n","for i, column in enumerate(df2.columns, 1):\n","    plt.subplot(4, 5, i)\n","    sns.boxplot(y=df2[column])\n","    plt.title(column)\n","plt.tight_layout()"],"metadata":{"id":"Vbn6rowXG5PM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"31Q14jDarEoA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Our dataset contain a lot of outliers. We have 3 options,\n","1. Remove outliers\n","2. Keeping random sample from the outliers to keep their representation intact.\n","3. Normalize using Robust Scaling then log transformation of features.\n","\n","We will go with the 3rd option -> Normalize using Robust Scaling then log transformation of features."],"metadata":{"id":"6CNj-U0RMpon"}},{"cell_type":"code","source":["\n","# Initialize RobustScaler\n","robust_scaler = RobustScaler()\n","\n","# Identify columns to scale\n","columns_to_scale =[col for col in df.columns if col != 'CHURNED']\n","\n","# Apply RobustScaler to the columns with outliers\n","df[columns_to_scale] = robust_scaler.fit_transform(df[columns_to_scale])"],"metadata":{"id":"xw3_twq8O_aX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.describe()"],"metadata":{"id":"_MijgF2iPRrg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Identify skewed columns\n","skewed_columns = ['TOTAL_POST_COUNT','TOTAL_LIKES_RECEIVED', 'TOTAL_COMMENTS_RECEIVED', 'TOTAL_LIKES_GIVEN', 'TOTAL_COMMENTS_WRITTEN', 'POST_TO_VISIT_RATIO', 'LIKE_TO_COMMENT_RATIO_GIVEN','LIKE_TO_COMMENT_RATIO_RECEIVED']\n","\n","# Apply log transformation to skewed features, adding 1 to shift from zero\n","for col in skewed_columns:\n","    df[col] = np.log1p(df[col])"],"metadata":{"id":"CXDDRMBfPQok"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.describe()"],"metadata":{"id":"o0eSJUVBQQHm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting box plots for each column to visually identify outliers\n","df2 = df.copy()\n","# df2 = df2[(df2.TOTAL_POST_COUNT\t< 20) & (df2.TOTAL_LIKES_RECEIVED\t< 20) & (df2.TOTAL_LIKES_GIVEN\t< 20) & (df2.TOTAL_COMMENTS_RECEIVED\t< 20) & (df2.TOTAL_COMMENTS_WRITTEN\t< 20)]\n","df2.drop('CHURNED',axis = 1,inplace=True)\n","plt.figure(figsize=(20, 15))\n","for i, column in enumerate(df2.columns, 1):\n","    plt.subplot(4, 5, i)\n","    sns.boxplot(y=df2[column])\n","    plt.title(column)\n","plt.tight_layout()"],"metadata":{"id":"VJWOTdRu7bCs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The outliers are handled pretty well and we are ready for the training!!"],"metadata":{"id":"m7vOt8vxQTxc"}},{"cell_type":"markdown","source":["# **Train and evaluate models**"],"metadata":{"id":"5wIQIlTyQZM7"}},{"cell_type":"markdown","source":["Let's separate label and feature sets"],"metadata":{"id":"R-c7s6TdSMz0"}},{"cell_type":"code","source":["# validation splitting\n","features = df.drop('CHURNED', axis=1)\n","labels = df['CHURNED'].astype(int)\n","\n","\n","# As these features tend to show too much direct correlation with the label, because these features are also part of the label criteria, so we drop them to prevent\n","# Also when these features are inlcuded, it makes the model acheive 100% scores, which is unbelieavable\n","features.drop(columns=['ENGAGEMENT_SCORE'],axis=1, inplace=True)\n","# features.drop(columns=['DAYS_SINCE_LAST_VISIT'],axis=1, inplace=True)\n","\n","\n","X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, random_state=42)"],"metadata":{"id":"t3ecQn-yQTWO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Define parameter grid\n","# param_grid = {\n","#     'n_estimators': [50, 100, 200],\n","#     'max_depth': [10, 20, 30],\n","#     'min_samples_split': [2, 5, 10],\n","#     'random_state': [42]\n","# }\n","\n","\n","# # Create a Random Forest Classifier\n","# rf_clf = RandomForestClassifier()\n","\n","# # Initialize GridSearchCV\n","# grid_search = GridSearchCV(estimator=rf_clf, param_grid=param_grid,\n","#                            scoring=make_scorer(accuracy_score), cv=5)\n","\n","# # Fit GridSearchCV\n","# grid_search.fit(X_train, y_train)\n","\n","# # Get best parameters and estimator\n","# best_params = grid_search.best_params_\n","# best_estimator = grid_search.best_estimator_\n","\n","# print(\"Best Parameters:\", best_params)"],"metadata":{"id":"4YzbUcUAJwaK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize and train the model\n","rf_clf = RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=2, random_state=42)"],"metadata":{"id":"tG4a9e5RSWPo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rf_clf.fit(X_train, y_train)"],"metadata":{"id":"fuDy9gxKSqCB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make predictions\n","y_pred = rf_clf.predict(X_val)\n","y_pred_proba = rf_clf.predict_proba(X_val)[:,1]"],"metadata":{"id":"v-PqXv2IS3VW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the model\n","precision = precision_score(y_val, y_pred)\n","recall = recall_score(y_val, y_pred)\n","accuracy = accuracy_score(y_val, y_pred)\n","f1 = f1_score(y_val, y_pred)\n","roc_auc = roc_auc_score(y_val, y_pred_proba)\n","\n","print(f'Validation Accuracy: {accuracy}')\n","print(f'F1 Score: {f1}')\n","print(f'ROC AUC Score: {roc_auc}')\n","print(f'Precision: {precision}')\n","print(f'Recall: {recall}')"],"metadata":{"id":"FBcyODUvXRIx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute ROC curve and AUC\n","fpr, tpr, _ = roc_curve(y_val, y_pred_proba)\n","roc_auc = auc(fpr, tpr)\n","\n","# Plot\n","plt.figure()\n","lw = 2\n","plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n","plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic')\n","plt.legend(loc='lower right')\n","plt.show()\n",""],"metadata":{"id":"oXMTfBBWJOy8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5TLITNgGWo7m"},"execution_count":null,"outputs":[]}]}